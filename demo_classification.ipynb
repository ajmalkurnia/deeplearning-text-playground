{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# This is a More Readable Demo and Detailed Process of Classification\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Load Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "# A two column CSV with header \"label & text\"\n",
    "DATA_PATH = \"INSERT YOUR DATA PATH HERE\"\n",
    "\n",
    "df = pd.read_csv(DATA_PATH)\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Preprocessing Text\n",
    "\n",
    "There are three preprocessing step that commonly used:\n",
    "1. Tokenization\n",
    "2. Remove Punctuation\n",
    "3. Casefolding\n",
    "\n",
    "There are also other preprocessing step such as:\n",
    "1. Lemmatization\n",
    "2. Stemming\n",
    "3. Stopword removal \n",
    "\n",
    "However, lemmatization, stemming, and stopword removal, removes bits of information of the text that you may want to keep for most of the task. So, these steps is mostly ignored when using deep learning model. \n",
    "\n",
    "__Note__: if you're using a pretrained network, make sure the preprocessing step is mathed with the preprocessing during pretraining"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1. Tokenization\n",
    "\n",
    "Tokenization is a process that splits an entire text into list of words (token). On some longer text, such as news article, you may want to split the text into paragraph first, then into sentences, and lastly split into token. For tokenization process, one of the most simple tokenization method is to split the text on the whitespace. On this demo the tokenization process will be using __tokenization__ module that on __nltk__. Specifically the __casual_tokenize__, a tokenization function to handle social media text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenization\n",
    "from nltk.tokenize import word_tokenize, casual_tokenize\n",
    "# casual_tokenize is a special tokenization function for social media text\n",
    "# word_tokenize uses PunktTokenizer for formal text\n",
    "df[\"tokenized_text\"] = df[\"text\"].apply(lambda t: casual_tokenize(t.lower()))\n",
    "df[\"tokenized_text\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2. Remove Punctuation (Recomended but Optional)\n",
    "\n",
    "With exception of some specific task, generally, punctuations does not provide a lot information of its text compared to words of the text. So, most of the time, the punctuation is removed from the text. The __punctuation__ function on __string__ package is a string of punctuations such as ```!\"#$%&\\'()*+,-./:;<=>?@[\\\\]^_`{|}~```. You could append this string with other punctuation character."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "\n",
    "\n",
    "def remove_punctuation(tokenized_text):\n",
    "    punctuations = string.punctuation\n",
    "    cleaned_text = []\n",
    "    for token in tokenized_text:\n",
    "        #         Remove punctuation\n",
    "        cleaned_token = token.translate(str.maketrans(\"\", \"\", punctuations))\n",
    "#         Filter out empty string and whitespace only token\n",
    "        if cleaned_token.strip():\n",
    "            cleaned_text.append(cleaned_token)\n",
    "    return cleaned_text\n",
    "\n",
    "\n",
    "df[\"clean_corpus\"] = [remove_punctuation(\n",
    "    data) for data in df[\"tokenized_text\"]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 Casefolding (Recommended but Optional)\n",
    "\n",
    "Same deal with punctuations, the difference between uppercase and lowercase is not really important for _most_ of text processing. Each different case of variation has to be process independently of each other and can take up extra slot vocabulary slot. Therefore, again with some exception, you may want to perform casefolding by lowering all of the text. (You can perform this __before__ tokenization)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"clean_corpus\"] = [[token.lower() for token in data]\n",
    "                      for data in df[\"clean_corpus\"]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4 Using Keras preprocessing module\n",
    "\n",
    "Keras package provides a  __text_to_word_sequence__ function that by default performs, tokenization with whitespace, remove punctuation, and casefolding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing.text import text_to_word_sequence\n",
    "\n",
    "df[\"alt_clean_corpus\"] = [text_to_word_sequence(text) for text in df[\"text\"]]\n",
    "df[\"alt_clean_corpus\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.5 Remove stopwords (Completely Optional)\n",
    "\n",
    "Stopwords is a group of words that does not add much meaning to the entire text, such as, this, that, is, at, on, etc. This step is mostly ignored because it may remove important information of the text that the Deep Learning model will try to learn. However, in some cases you may want to perform stopword removal such as low-resource settings (low data count) when you have to put lazer focus on the important bit of the text given the limited resources.\n",
    "\n",
    "The nltk provides stopwords list of some languages. You could also take 0.x% of most common word of some big corpus (wiki) as stopwords. Alternatively, you use your own stopword list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "# Open stopword list\n",
    "stopword_list = stopwords.words(\"indonesian\")\n",
    "\n",
    "df[\"clean_corpus\"] = [list(filter(lambda x: x not in stopword_list, data))\n",
    "                      for data in df[\"clean_corpus\"]]\n",
    "df[\"clean_corpus\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.6. Data Split\n",
    "\n",
    "After all of data is cleaned the last step is to split the dataset. The data can be split either in to three split (train, validation(dev), and test) or two split (train, test). if you think the dataset is too low you can perform K-Folding split. This demo will split data in to three split, 20% for testing, 8% (10% of 80%) for validation, and the rest for training. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X = df[\"clean_corpus\"].values.tolist()\n",
    "y = df[\"label\"].values.tolist()\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, train_size=0.8, test_size=0.2, random_state=4371\n",
    ")\n",
    "X_train, X_val, y_train, y_val = train_test_split(\n",
    "    X_train, y_train, train_size=0.9, test_size=0.1, random_state=4371\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Training model\n",
    "\n",
    "All of the classifier implemented in this repo, has some hidden preprocessing steps that usually you have to do when using keras, input + label vectorization, and embedding initialization. Before getting into the classifier, this demo will explain both processes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.-2 Label Vectorization\n",
    "\n",
    "Usually the label vectorizatino is done by converting the numeric label into one hot vector, an easy way to do this is to use __to_categorical__ function on keras package. but first, built two dictionary of the index of the label and its inverse index."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.utils import to_categorical\n",
    "\n",
    "\n",
    "def vectorized_label(label_data, label_index):\n",
    "    return to_categorical([label_index[label] for label in label_data])\n",
    "\n",
    "\n",
    "# Get all of unique label\n",
    "label = sorted([*{*y_train}])\n",
    "# Make label index (to convert label to numeric)\n",
    "label_index = {ch: idx for idx, ch in enumerate(label)}\n",
    "# Make inverse label index (to convert back prediction result to string label)\n",
    "inverse_label_index = {idx: ch for ch, idx in label_index.items()}\n",
    "\n",
    "vectorized_y_train = vectorized_label(y_train, label_index)\n",
    "vectorized_y_val = vectorized_label(y_val, label_index)\n",
    "vectorized_y_test = vectorized_label(y_test, label_index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.-1 Input Vectorization\n",
    "\n",
    "Input vectorization is basically a process to convert the input from text sequence into numerical sequences. This process is done as follows:\n",
    "1. Determine how many words you want to represent from you training dataset (x).\n",
    "2. Build a frequency table of each token.\n",
    "3. Takes x most common words from the frequency table.\n",
    "4. Make an inverse index of the chosen vocabulary. Start from 1 because 0 is used for padding/unkown words.\n",
    "5. Convert the input using the inverse index, set 0 for words not on the index."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict  # For frequency table\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def make_inverse_index(data_train, vocab_size):\n",
    "    #     Make frequency table\n",
    "    frequency_table = defaultdict(int)\n",
    "    for data in data_train:\n",
    "        for token in data:\n",
    "            frequency_table[token] += 1\n",
    "\n",
    "#     Sort by frequency\n",
    "    sorted_vocab = sorted(\n",
    "        frequency_table.items(), key=lambda x: x[1], reverse=True\n",
    "    )\n",
    "#     List of chosen vocabulary\n",
    "    vocab = [\"UNK\"]\n",
    "    for idx, word in enumerate(sorted_vocab):\n",
    "        if len(vocab) < vocab_size:\n",
    "            vocab.append(word[0])\n",
    "#     Inverse index\n",
    "    return {word: idx for idx, word in enumerate(vocab)}\n",
    "\n",
    "\n",
    "def input_vectorization(corpus, inverse_index, max_length):\n",
    "    #     Initialize zero matrix with the size of [#data x max_length]\n",
    "    #     this way we already performed a padding or truncate any data with length over max_length\n",
    "    vectorized_input = np.zeros(\n",
    "        shape=(len(corpus), max_length), dtype=np.int32)\n",
    "    for idx, data in enumerate(corpus):\n",
    "        for jdx, token in enumerate(data):\n",
    "            if jdx < max_length:\n",
    "                vectorized_input[idx][jdx] = inverse_index.get(token, 0)\n",
    "    return vectorized_input\n",
    "\n",
    "\n",
    "# Determine how many vocabulary and maximum length of the sequences\n",
    "VOCAB_SIZE = 5000\n",
    "MAXIMUM_LENGTH = 50\n",
    "inverse_index = make_inverse_index(X_train, VOCAB_SIZE)\n",
    "\n",
    "vectorized_x_train = input_vectorization(\n",
    "    X_train, inverse_index, MAXIMUM_LENGTH)\n",
    "vectorized_x_val = input_vectorization(X_val, inverse_index, MAXIMUM_LENGTH)\n",
    "vectorized_x_test = input_vectorization(X_test, inverse_index, MAXIMUM_LENGTH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.0 Embedding\n",
    "\n",
    "Embedding is vector representation of a word. There are various ways to obtain this representation. Some notes on embedding:\n",
    "\n",
    "- The simplest one would be letting the Embedding layer generate a random vector for each unique word (forming a matrix the size of ( _VOCAB_SIZE_ x _EMBEDDING_DIMENSION_SIZE_ )) and let the training process adjust the representation of the word. However, this would need a lot of data and training epoch to give a proper representation of the word. \n",
    "- Another simple way to represent a word is by using one hot embedding (similar to label vectorization). This embedding is generated with identity matrix with the size of ( _VOCAB_SIZE_ x  _VOCAB_SIZE_ ) and set the initial weight of the Emebdding layer with this matrix.\n",
    "- Most of the input representation of text is using a pretrained word embedding that is trained by various word embedding method such as __Word2Vec__, __FasText__, or __GLoVe__ on relatively big text corpus. The embedding is a matrix with the size of ( _VOCAB_SIZE_ x _WORD_EMBEDDING_SIZE_ ), with each row denotes a word vector from pretrained word embedding. A random vector will be generated if a word in vocabulary does not exist in the pretrained word emebdding. Same as one hot embedding, the resulted matrix would be used to initialize the weight of Embedding layer.\n",
    "- The newest way to set initial weight on the embedding layer is by sending the metrix inside a __Constant__ class on __keras.initializer__ to set initial weight of Embedding layer (__embedding_initializer__ parameter). Some article would tell you to use the __weight__ parameter on embedding layer, this works on older version of keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.initializers import Constant\n",
    "\n",
    "onehot_embedding = np.eye(VOCAB_SIZE)\n",
    "onehot_embedding_weight = Constant(onehot_embedding)\n",
    "# Send this to the embedding layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 Using the classifier\n",
    "\n",
    "There are some classifier that has been implemented in this repo, such as, CNN, RNN, RNN+Attention, Transformer, and HAN. Each classifier has their own parameter and a set of generic parameter that applies to all classifier. the parameter is:\n",
    "```\n",
    "input_size      : int, maximum number of token input\n",
    "optimizer       : string, learning optimizer, keras model compile \"optimizer\" parameter\n",
    "loss            : string, loss function, keras model compile \"loss\" parameter\n",
    "embedding_matrix: numpy array, custom embedding matrix of the provided vocab\n",
    "vocab size      : int, maximum size of vocabulary of the model\n",
    "                  (most frequent word of the training data will be used)\n",
    "                  set to 0 to use every word in training data\n",
    "vocab           : dictionary, a vocab inverse index.\n",
    "embedding_file  : string, path to pretrined word embedding file\n",
    "embedding_type  : string, type of embedding, available options\n",
    "                  w2v for zword2vec, matrix will be taken from embedding file\n",
    "                  ft for FasText, matrix will be taken from embedding file\n",
    "                  onehot, initialize one hot encoding of vocabulary\n",
    "                  custom, use embedding matrix\n",
    "                  or any valid keras.initializer string\n",
    "train_embedding : boolean, determine whether the Embedding layer should be trainable\n",
    "                  which apparently not recommended when using pretrained weight\n",
    "                  refer -> https://keras.io/examples/nlp/pretrained_word_embeddings/\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "\n",
    "\n",
    "def run_classifier(classifier_class, data):\n",
    "    X_train, y_train, X_test, y_test, X_val, y_val = data\n",
    "    classifier_class.train(\n",
    "        X_train, y_train, 10, 32, (\n",
    "            X_val, y_val\n",
    "        )\n",
    "    )\n",
    "    prediction = classifier_class.test(X_test)\n",
    "    print(classification_report(y_test, prediction))\n",
    "\n",
    "\n",
    "PRETRAINED_PATH = \"INSERT THE PRETRAINED WORD EMBEDDING PATH HERE\"\n",
    "base_classifier_parameter = {\n",
    "    \"input_size\": 50,\n",
    "    \"optimizer\": \"adam\",\n",
    "    \"loss\": \"categorical_crossentropy\",\n",
    "    \"embedding_matrix\": None,\n",
    "    \"vocab_size\": 5000,\n",
    "    \"vocab\": None,\n",
    "    \"embedding_file\": PRETRAINED_PATH,\n",
    "    \"embedding_type\": \"ft\",\n",
    "    \"train_embedding\": False\n",
    "}\n",
    "grouped_data = X_train, y_train, X_test, y_test, X_val, y_val"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.1 CNN\n",
    "CNN speific parameter\n",
    "```\n",
    "conv_layers: list of tupple, list of parameter for convolution layers,\n",
    "             each tupple for one convolution layer that consist of : [\n",
    "                (int) Number of filter,\n",
    "                (int) filter size,\n",
    "                (int) maxpooling (-1 to skip),\n",
    "                (string) activation function\n",
    "             ]\n",
    "fcn_layers : list of tupple, list of parameter for Dense layers,\n",
    "                each tupple for one FC layer,\n",
    "                final layer (softmax) will be automatically added in,\n",
    "                each tupple consist of: [\n",
    "                    (int) Number of unit,\n",
    "                    (float) dropout (-1 to skip),\n",
    "                    (string) activation function\n",
    "                ]\n",
    "conv_type  : string, Set how the convolution will be performed, available options: parallel/sequence\n",
    "             parallel: each cnn layer from conv_layers will run against\n",
    "                 embedding matrix directly, the result will be concatenated before FCN layer\n",
    "                 Refer to Yoon Kim, 2014\n",
    "             sequence: cnn layer from conv_layers will stacked sequentially,\n",
    "                commonly used for character level CNN, on word level CNN 'parallel' is recommended\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from model.CNNText.cnn_classifier import CNNClassifier\n",
    "\n",
    "cnn_parameter = {key: value for key, value in base_classifier_parameter.items()}\n",
    "cnn_parameter[\"conv_layers\"] = [\n",
    "    (256, 3, 1, \"relu\"),\n",
    "    (256, 4, 1, \"relu\"),\n",
    "    (256, 5, 1, \"relu\")\n",
    "]\n",
    "cnn_parameter[\"fcn_layers\"] = [(512, 0.2, \"relu\")]\n",
    "cnn_parameter[\"conv_type\"] = \"parallel\"\n",
    "\n",
    "\n",
    "cnn_classifier = CNNClassifier(**cnn_parameter)\n",
    "run_classifier(cnn_classifier, grouped_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.2 RNN & RNN + Attention\n",
    "\n",
    "Parameter\n",
    "```\n",
    "rnn_size  : int, RNN hidden unit\n",
    "dropout   : float, [0.0, 1.0] dropout just before softmax layer\n",
    "rnn_type  : string, RNN memory type, \"gru\"/\"lstm\"\n",
    "attention : string, attention scoring type choice available:\n",
    "            dot/scale/general/location/add/self,\n",
    "            set None to not use attention mechanism\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from model.RNNText.rnn_classifier import RNNClassifier\n",
    "rnn_parameter = {key: value for key, value in base_classifier_parameter.items()} \n",
    "rnn_parameter[\"rnn_size\"] = 100\n",
    "rnn_parameter[\"dropout\"] = 0.2\n",
    "rnn_parameter[\"rnn_type\"] = \"lstm\"\n",
    "rnn_parameter[\"attention\"] = \"self\"\n",
    "\n",
    "rnn_classifier = RNNClassifier(**rnn_parameter)\n",
    "run_classifier(rnn_classifier, grouped_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.3 Transformer\n",
    "\n",
    "Parameter\n",
    "```\n",
    "n_blocks          : int, number of transformer stack\n",
    "dim_ff            : int, hidden unit on fcn layer in transformer\n",
    "dropout           : float, dropout value\n",
    "n_heads           : int, number of attention heads\n",
    "attention_dim     : int, number of attention dimension\n",
    "                    value will be overidden if using custom/pretrained embedding matrix\n",
    "pos_embedding_init: bool, Initialize posiitonal embedding with\n",
    "                    sincos function, or else will be initialize with glorot_uniform\n",
    "fcn_layers        : list of tupple, configuration of each\n",
    "                    fcn layer after transformer, each tupple consist of:\n",
    "                        [int] number of units,\n",
    "                        [float] dropout after fcn layer,\n",
    "                        [string] activation function\n",
    "sequence_embedding: string, a method how to get representation of entire sequence,\n",
    "                    the representation will be used for the input of FCN layer, available option:\n",
    "                    cls, prepend [CLS] token in the sequence, then take\n",
    "                        attention output of [CLS] token as sequence representation (BERT style)\n",
    "                    global_avg, use GlobalAveragePool1D\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from model.TransformerText.transformer_classifier import TransformerClassifier\n",
    "\n",
    "transformer_parameter = {key: value for key, value in base_classifier_parameter.items()}\n",
    "transformer_parameter[\"n_blocks\"] = 2\n",
    "transformer_parameter[\"dim_ff\"] = 256\n",
    "transformer_parameter[\"dropout\"] = 0.3\n",
    "transformer_parameter[\"n_heads\"] = 8\n",
    "transformer_parameter[\"attention_dim\"] = 256\n",
    "transformer_parameter[\"pos_embedding_init\"] = True\n",
    "transformer_parameter[\"fcn_layers\"] = [(128, 0.1, \"relu\")]\n",
    "transformer_parameter[\"sequence_embedding\"] = \"global_avg\"\n",
    "\n",
    "transformer_classifier = TransformerClassifier(**transformer_parameter)\n",
    "run_classifier(transformer_classifier, grouped_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.4 Hierarchical Attention Network (HAN)\n",
    "\n",
    "The original paper use this for document classification tasks, however we still could use them for sentence classification by attending to the character and token to determine the class of a sentence. Unlike other network so far, HAN accepts a 3D input array. For documenet classification, we could see this as an entire text split by sentence, and each sentences split by token. In term of sentence classification, we could see it as text split by its token, and each token split by the characters.\n",
    "\n",
    "Parameter -> Similar to RNN\n",
    "```\n",
    "input_shape : 2 length tuple (int, int), maximum input shape,\n",
    "              the first element refer to maximum length of a data or maximum number of sequence in a data\n",
    "              the second element refer to maximum length of a sequence or maximum number of sub-sequence in\n",
    "              a sequence\n",
    "              NOTE this will override whatever value that passed by input_size parameter.\n",
    "rnn_size    : int, number of rnn hidden units\n",
    "dropout     : float, dropout rate (before softmax)\n",
    "rnn_type    : string, the type of rnn cell, available option:\n",
    "              gru or lstm\n",
    " ```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from model.RNNText.han_classifier import HANClassifier\n",
    "han_parameter = {key: value for key, value in base_classifier_parameter.items()}\n",
    "han_parameter[\"rnn_size\"] = 100\n",
    "han_parameter[\"dropout\"] = 0.2\n",
    "han_parameter[\"rnn_type\"] = \"lstm\"\n",
    "han_parameter[\"input_shape\"] = (25, 10)\n",
    "\n",
    "grouped_data = list(grouped_data)\n",
    "grouped_data[0] = [[[*token] for token in doc] for doc in grouped_data[0]]\n",
    "grouped_data[2] = [[[*token] for token in doc] for doc in grouped_data[2]]\n",
    "grouped_data[4] = [[[*token] for token in doc] for doc in grouped_data[4]]\n",
    "\n",
    "han_classifier = HANClassifier(**han_parameter)\n",
    "run_classifier(han_classifier, grouped_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#### Next Classifier RCNN"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.6.9 64-bit",
   "language": "python",
   "name": "python36964bita5ef3e741f084861b81aecbe4672bddc"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
